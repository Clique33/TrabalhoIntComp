\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{color}
\usepackage[boxed,portuguese]{algorithm2e}
\usepackage[portuguese]{babel}

\title{Iterated Local Search no Problema da Clique Máxima}
\author{Gabriel Cardoso de Carvalho}
\date{}

\begin{document}
\maketitle

\textbf{Resumo:} Esse artigo mostra os resultados de uma implementação do Iterated Local Search no problema da Clique Máxima baseada em decisões aleatórias com o objetivo de comparar com os resultados de outras implementações disponíveis na literatura para as instâncias do DIMACS. Os resultados são, como esperado, piores do que em soluções gulosas, porém não são tão ruins quanto espera-se. Aqui comparamos o nosso algoritmo com o IKLS \cite{kopt}, o HSSGA \cite{HSSGA} e o VNS para cliques máximas \cite{vns}. De fato, o algoritmo erra somente nas instâncias mais complexas, em que outras metaheurísticas como o HSSGA e o VNS também erram, e nos casos em que somente ele erra, ele o faz por uma margem bem pequena.

\section{Introdução}

O problema de encontrar a Clique Máxima (CM) é extremamente conhecido e estudado, pois inúmeros problemas práticos de diversas áreas diferentes, como biologia computacional, economia e análise de redes sociais podem ser modelados como CM. 
Além disso, a sua versão de decisão foi um dos primeiros problemas a serem provados NP-Completos.\par 

Ele pode ser definido da seguinte maneira, seja o grafo $G=(V,E)$ onde $V = 1,2, ... , n$ é o conjunto de vértices e $E \subseteq V \times V$ é o conjunto de arestas, uma Clique $C \subseteq V $é tal que $\forall i,j \in C, (i,j) \in E$ 
, ou seja, todos os vértices em $C$ são adjacentes entre si. Ou ainda, $C$ é um subgrafo completo de $G$. O problema da clique máxima é o problema de encontrar a clique de cardinalidade máxima do grafo $G$.\par

Diversas soluções foram propostas, tanto métodos exatos quanto heurísticas e metaheurísticas \cite{review,pardaloshand,DIMACS2}. As propostas no geral tendem a utilizar o sistema \textit{breach and bound} nos métodos exatos e heurísticas gulosas em buscas locais, preferindo vértices de maior grau. De maneira geral, os algoritmos gulosos partem de uma clique $C$ inicial que contém apenas um vértice e um conjunto $N_C$ de vértices $v \in V$ que são os vértices vizinhos à $C$, ou seja, $\forall u \in C, v$ é adjacente à $u$. Daí o algoritmo adiciona vértices de  $N_C$ em $C$, escolhendo sempre o vértice de $N_C$ que tem o maior grau no subgrafo $G(N_C)$, até que $C$ seja \textit{maximal}, ou seja, até que não exista uma clique $C'$ maior que $C$ tal que $C \subseteq C'$. Em outros trabalhos esse método é chamado de \textit{Busca Local 1-opt } \cite{KLS}.\par

A metaheurística implementada nesse artigo é a \textit{Iterated Local Search (ILS)}, que pode ser resumida como uma metaheurística que cria, de maneira iterativa, uma sequência de soluções geradas por uma heurística interna (ou busca local) \cite{handbook}. É esperado que as soluções providas pelo ILS sejam melhores do que uma simples repetição da heurística de maneira aleatória.

Nesse artigo é proposta uma implementação do ILS focada na aleatoriedade, de modo a comparar seu desempenho com métodos gulosos, como a implementação do IKLS de \textit{Katayama} \cite{kopt}, que utiliza a \textit{Busca Local k-opt (KLS)} \cite{KLS} como busca local, que é uma generalização da busca local 1-opt, onde adiciona-se $k$ vértices à clique por vez, permitindo retirar vértices da clique para isso, de maneira dinâmica, ou seja, o $k$ não é fixo,  e uma perturbação baseada na \textit{menor conecividade por arestas (LEC-KLS)}, onde escolhe-se um vértice $v$ que não pertence à $C$, de maneira que $v$ seja adjacente à menor quantidade de vértices de $C$. Então, adiciona-se $v$  à $C$ e remove de $C$ todos os vértices que não são vizinhos à $v$.\par

A seção 2 apresenta como foi feita a implementação do ILS, enquanto a seção 3 cobre toda a implementação e os resultados experimentais do método sobre as instâncias do DIMACS \cite{DIMACS2}. A seção 4 Apresenta as conclusões e os trabalhos futuros.

\section{ILS}


Esta implementação do ILS segue o padrão de Glover \cite{handbook} utilizando o algoritmo 1, onde \textit{GeraSolucaoInicial} trata-se da função que retorna uma clique maximal, a \textit{BuscaLocal} parte de uma clique maximal e busca nas vizinhanças uma clique maior, \textit{Perturbacao} recebe uma clique maximal e retira uma quantidade aleatória de vértices dessa clique, em alguns casos retirando todos os vértices, caso em que ocorre uma reinicialização, e \textit{CriterioAceitacao} que escolhe se a próxima perturbação será feita na solução antiga ou na atual.\par


\begin{algorithm}
 \KwData{Grafo $G$, inteiro $n_{iter}$}
 $s_0$ = GeraSolucaoinicial($G$)\;
 $s*$ = BuscaLocal($s_0$)\;
 $k$ = 0\;
 \While{$k$ for menor que $n_{iter}$}{
  $s'$ = Perturbacao($s*$)\;
  $s*'$ = BuscaLocal($s'$)\;
  $s*$ = CriterioAceitacao($s*, s*'$)\;
 $k_{++}$\;
 }
 \caption{Estrutura do ILS}
\end{algorithm}

Os detalhes de cada função são descritos nas subseções seguintes. 

\subsection{Geração da Solução Inicial}

A função \textit{GeraSolucaoInicial($G$)} somente escolhe um vértice aleatório $v$ de $G$ e chama a função \textit{geraSolucao($G$, v)}.\par

A função \textit{geraSolucao($G$, v)} gera a solução inicial ao criar uma clique $C$ de tamanho 1 utlizando o vértice $v$. Essa clique é um estrutura que contém os vértices que a compõe (denotados por $C$), o seu tamanho $k$, e um conjunto $N_C$ de vértices que podem ser adicionados à ela. A partir daí, adiciona-se vértices aleatórios de $N_C$ à $C$ e atualiza seu tamanho e $N_C$ até que a clique $C$ seja maximal, como mostra o algoritmo 2.\par

\begin{algorithm}
 \KwData{Grafo $G$, vértice $v$}
 $s_0$ = clique contendo $v$\;
 $N_{s_0}$ = $V_v$\;
 $k$ = 1\;
 \While{$N_C \neq \emptyset$}{
  $u$ = vértice aleatório de $N_C$\;
  $s_0$ =$s_0 + u$\;
  $N_{s_0}$ = $N_{s_0} \cap V_u$\;
 $k_{++}$\;
 }
 \KwRet{clique $s_0$ maximal}
 \caption{função geraSolucao}
\end{algorithm}

No algoritmo 2, $N_{s_0}$ representa os vértices que podem ser adicionados na clique $s_0$, enquanto $V_v$ representa os vértices adjacentes ao vértice $v$. O algoritmo para quando não há mais nenhum vértice que possa ser adicionado a $C$.\par

Além de ser utilizada para criar a solução inicial, a função \textit{GeraSolucaoInicial($G$)} é chamada novamente sempre que a Perturbação decide que deve ser feita uma reinicialização.

\subsection{Busca Local}

A busca local vai receber uma clique $C$, e a partir dessa clique, vai tentar melhorá-la olhando para as vizinhanças de $C$. Temos duas vizinhanças, $N_1$ e $N_2$ que são explicadas mas a frente. Elas são conjuntos de cliques vizinhas a $C$, dada alguma propriedade. Dados esses conjuntos, a busca local irá simplesmente maximizar cada uma das cliques e escolher uma delas.\par

Definimos aqui $N_i(C)$ como a vizinhança da clique $C$, tal que, $\forall C' \in N_i(C), $\\$C' \subset C,  k(C') = k(C)-i$. Portanto as vizinhanças que usaremos, $N_1$ e $N_2$, são subcliques de $C$ com tamanhos $k-1$ e $k-2$, respectivamente, onde $k$ é o tamanho da clique $C$.\par

$N_1$ tem tamanho $k$, ou seja, temos um vizinho para cada vértice de $C$. Para $N_2$, por outro lado temos $\binom{k}{2}$  vizinhos, ou ainda, $\frac{k^2 - k}{2}$. Dessa forma, é possível utilizar a técnica de \textit{Best Improvement} (testar todos e escolher o melhor) para $N_1$. Caso $N_1$ não produza uma melhora, então testa-se para $N_2$, onde se faz necessário utilizar \textit{First Improvement} (escolher o primeiro que melhorar a solução atual) devido ao seu tamanho. O algoritmo 3 mostra o pseudo-código da busca local.\par

\begin{algorithm}
 \KwData{Clique $s$}
 $N_1$ = conjunto de subcliques de $s$ de tamanho $k(s)-1$\;
 $s'$ = s\;
 
\For{$c \in N_1$}{
  $c$ = maximiza($c$)\;
  \uIf{$k(c) > k(s')$}{
   $s'$ = $c$\;
  }
 }

  \uIf{$k(s') > k(s)$}{
   \KwRet{clique $s'$}\;

  }

 $N_2$ = conjunto de subcliques de $s$ de tamanho $k(s)-2$\;

\For{$c \in N_2$}{
  $c$ = maximiza($c$)\;
  \uIf{$k(c) > k(s')$}{
   $s'$ = $c$\;
  }
 }

 \KwRet{clique $s$}
 \caption{BuscaLocal}
\end{algorithm}

As cliques, tanto em $N_1$ quanto $N_2$, são proibidas de voltarem às suas cliques originais, ou seja, seja $c \in N_1$ se $c = s + u$, onde $s$ é a clique original de $c$ e $u$ é o vértice de $s$ que não está em $c$, então $u$ não faz parte de $N_C(c)$.\par

 Buscar todo o $N_2$ implica em complexidades de tempo e espaço grandes demais em grafos muito grandes, como as instâncias \textit{C4000.5}, \textit{MANN\_a81} e \textit{keller6} do DIMACS \cite{DIMACS2}. Dessa forma, como o tamanho de $N_2$ é muito grande, calcula-se apenas $k$ dos $\frac{k^2-k}{2}$ elementos de $N_2$ de maneira aleatória.\par

A busca local portanto sempre vai retornar uma clique maior que a entrada dela, ou a própria entrada caso não consiga melhorá-la. 

\subsection{Perturbação}

A perturbação simplesmente sorteia um valor $2 \leq n \leq k$ e remove de $s*$ $n$ vértices aleatórios. Caso $n = k$, então é feita a reinicialização do procedimento, retornando a chamada da função geraSolucao($v$), onde $v$ é um vértice aleatório do grafo $G$. Caso $n < k$ então simplesmente $s'$ recebe $s*$ e depois são removidos os $n$ vértices de $s'$ . Depois de removidos os $n$ vértices, atualiza-se $N_s'$ maximiza a solução de maneira aleatória. Algoritmo 4 ilustra esse processo.\par

\begin{algorithm}
 \KwData{clique $s*$}
 $n$ = sorteie um número $\in \{2,k(s*)\}$\;
 \uIf{$n = k$}{
   $v$ = vértice aleatório $\in V(G)$\;
   \KwRet{geraSolucao($v$)}
  }
 $s'$ = $s*$\;
 \While{$k(s') > k(s*)-n$}{
   $v$ = vértice aleatório $\in s'$
  $s'$ = $s' - v$\;
  $k(s')_{--}$\;
 }
 recalcula $N_{s'}$\;
 \KwRet{s'}
 \caption{Perturbação}
\end{algorithm}


É feita dessa forma pois a perturbação deve fugir do ótimo local e buscar nas vizinhanças uma solução melhor. Dessa forma, a perturbação não pode ser muito forte ou muito fraca. No caso deste artigo, a força da perturbação é decidida de maneira aleatória, mas somente ocorre uma reincialização com probabilidade $p = \frac{1}{k-1}$, assim como ele aplicará uma leve perturbação com a mesma probabilidade. Na média, a perturbação terá força entre os dois extremos.

\subsection{Critério de Aceitação}

O critério de aceitação tem o dever de escolher qual solução sofrerá a próxima perturbação. É bem simples e tem três opções: retorna o maior entre $s*$ e $s*'$, sempre retorna $s*'$, ou , sorteia um dos dois para ser o retorno. \par

O primeiro caso é o caso em que sempre retorna-se o melhor dos dois, que seria o guloso, enquanto o segundo é uma maneira também chamada de \textit{random walk}, pois simplesmente caminha para a próxima solução sem se preocupar com a qualidade dela. O terceiro caso seria o meio termo entre os dois mas que ainda corre o risco de escolher uma solução pior sem nenhum ganho no resto do procedimento.\par

Nos experimentos apenas o primeiro caso é utilizado pois é certo que nenhum deles apresenta ganho de velocidade no procedimento geral em comparação com o outro. 

\section{Resultados Experimentais}

\begin{center}
\begin{table}
\begin{tabular}{| l | l | l | l | l | l |}
    \hline
    Instances			& 	DIMACS		& Maior		& Média(M)		& Tempo	& Média(T)\\ \hline
C125.9				&	34			&34			&33.866665		&22.069		&1.4712666\\ \hline
C250.9				&	44			&44			&42.2			&79.872		&5.3248\\ \hline
C500.9				&	57			&53			&51.066666		&324.396	&21.6264\\ \hline
C1000.9				&	68			&62			&59.133335		&1413.653	&94.24353\\ \hline
C2000.9 			&	80			&70			&66.46667		&6268.48	&417.89865\\ \hline	
DSJC1000\_5		&	15			&14			&13.133333		&111.005	&7.400333\\ \hline
DSJC500\_5			&	13			&13			&12.133333		&34.25		&2.2833333\\ \hline
C2000.5				&	16			&15			&14.266666		&374.985	&24.998999\\ \hline
C4000.5 			&	18			&16			&15.4			&4310.259	&287.3506\\ \hline
MANN\_a27			&	126			&125		&123.933334	&1545.286	&103.019066\\ \hline
MANN\_a45			&	345			&336		&334.888888	&48895.62	&5432.82\\ \hline
MANN\_a81			&	1100		&1084		&-				&-			&-\\ \hline
brock200\_2			&	12			&12			&10.666667		&5.219		&0.34793332\\ \hline
brock200\_4			&	17			&16			&15.266666		&8.829		&0.58860004\\ \hline
brock400\_2			&	29			&24			&22.6			&59.105		&3.9403334\\ \hline
brock400\_4			&	33			&25			&22.666666		&61.049		&4.0699334\\ \hline
brock800\_2			&	24			&20			&18.933332		&135.98		&9.065333\\ \hline
brock800\_4			&	26			&19			&18.466667		&133.014	&8.8676\\ \hline
gen200\_p0.9\_44	&	44			&44			&39.266666		&36.969		&2.4646	\\ \hline
gen200\_p0.9\_55	&	55			&55			&47.0			&51.641		&3.4427333\\ \hline
gen400\_p0.9\_55	&	55			&50			&47.6			&157.595	&10.506333\\ \hline
gen400\_p0.9\_65	&	65			&65			&50.333332		&156.574	&10.438267\\ \hline
gen400\_p0.9\_75	&	75			&75			&59.733334		&213.645	&14.243\\ \hline
hamming10-4		&	40			&40			&37.133335		&417.751	&27.850067\\ \hline
hamming8-4			&	16			&16			&16.0			&12.258		&0.8172\\ \hline
keller4				&	11			&11			&11.0			&4.891		&0.32606664\\ \hline
keller5				&	27			&27			&24.4			&228.479	&15.231934\\ \hline
keller6				&	59			&52			&49.4			&8234.637	&548.97577\\ \hline
p\_hat300-1			&	8			&8 			&8.0			&3.14		&0.20933335\\ \hline
p\_hat300-2			&	25			&25			&25.0			&28.312		&1.8874667\\ \hline
p\_hat300-3			&	36			&36			&34.733334		&58.169		&3.8779333\\ \hline
p\_hat700-1			&	11			&11			&9.333333		&19.366		&1.2910666\\ \hline
p\_hat700-2			&	44			&44			&43.533333		&274.399	&18.293266\\ \hline
p\_hat700-3			&	62			&62			&61.266666		&498.792	&33.2528\\ \hline
p\_hat1500-1		&	12			&12			&10.4			&94.455		&6.297\\ \hline
p\_hat1500-2		&	65			&65			&63.666668		&1571.55	&104.770004\\ \hline
p\_hat1500-3		&	94			&93			&91.2			&4536.831	&302.4554\\ \hline
    \hline
\end{tabular}
\caption{Tabela com os resultados do procedimento nas 37 instâncias do DIMACS}
\end{table}
\end{center}

Toda a implementação foi feita na linguagem JAVA e pode ser acessado por git \cite{git}. Diversos testes foram feitos nas instâncias do DIMACS, com foco nos mais simples, pois é possível testar mais vezes já que essas instâncias são menores.\par

A implementação tem somente um parâmetro, $nIter$ que foi setado como $nIter = 150$, pois abaixo disso alguns testes mostraram que começava a achar cliques muito menores, enquanto maior que 150 não demonstrou melhoria significativa em comparação. Cada um dos testes foram repetidos 15 vezes para cada instância, com exceção de MANN\_a45 (que foi rodado apenas 9 vezes), e MANN\_81 que rodou apenas GeraSolucaoInicial. \par

Essas instâncias não foram completas pois o tamanho das cliques era muito grande, $> 300$ para MANN\_a45  e $>1000$ para MANN\_a81, e por isso, $N_1$ e $N_2$ ficaram muito grandes. O gargalo fica no recálculo de $N_s$, que precisa checar todos os vizinhos de todos os vértices em $N_1$ e $N_2$. A tabela 1 abaixo mostra os resultados de todos os experimentos.\par

Na tabela 1, \textit{Instância} representa a instância em que o experimento foi feito, \textit{DIMACS} representa o maior valor que foi encontrado pelo DIMACS \cite{DIMACS2} para a dada instância, \textit{Maior} representa a maior clique encotrada pelo nosso algoritmo, \textit{Média(M)} representa o tamanho médio das cliques encontradas em cada uma das 15 tentativas, \textit{Tempo} representa o tempo que levou para todo o experimento ser completo e \textit{Média(T)} representa o tempo médio que cada uma das 15 tentativas levou em média para completar as 150 iterações.\par

Quando comparados com os melhores resultados do DIMACS, vemos que o ILS proposto se iguala ao melhor encontrado em 20 ocasiões das 37. Quando erra, por outro lado, tem erro médio de 10\% comparado ao valor do DIMACS, sendo o maior erro de 27\% na instância brock800\_4 e o menor é de 1\% nas instâncias MANN\_a27, MANN\_a81 (lembrando que nesta instância não houve sequer a buscal local) e p\_hat1500-3. Entre os erros é válido colocar que nas instâncias DSJC1000\_5, C2000.5, MANN\_a27, brock200\_4 e p\_hat1500-3, a diferença de tamanho entre a maior clique conhecida pelo DIMACS e a nossa foi de apenas um vértice.\par

A tabela 2 mostra a comparação do nosso algoritmo (na tabela está como ILS) com alguns outros algoritmos da literatura, o IKLS \cite{kopt}, VNS para clique máxima \cite{vns} e o HSSGA \cite{HSSGA}.\par

É possível notar que nosso algoritmo se aproxima do HSSGA, sendo que sempre que o HSSGA erra o nosso também erra. Isso implica na possibilidade de que os grafos que são difíceis para o HSSGA são também difíceis para o nosso, enquanto o VNS erra somente  no brock800\_2 e brock800\_4 que são duas das instâncias que o nosso algoritmo errou com maior diferença. 

\begin{center}
\begin{table}
\begin{tabular}{| l | l | l | l | l | l |}
    \hline
    Instances			& 	DIMACS		& ILS		& IKLS\cite{kopt}	&VNS\cite{vns}	& HSSGA\cite{HSSGA}\\ \hline
C125.9				&	34			&34			&34				&34				&34\\ \hline
C250.9				&	44			&44			&44				&44				&44\\ \hline
C500.9				&	57			&53			&57				&57				&56\\ \hline
C1000.9				&	68			&62			&68				&68				&66\\ \hline
C2000.9 			&	80			&70			&78				&78				&74\\ \hline	
DSJC1000\_5		&	15			&14			&15				&15				&15\\ \hline
DSJC500\_5			&	13			&13			&13				&13				&13\\ \hline
C2000.5				&	16			&15			&16				&16				&16\\ \hline
C4000.5 			&	18			&16			&18				&18				&17\\ \hline
MANN\_a27			&	126			&125		&126			&126			&126\\ \hline
MANN\_a45			&	345			&336		&345			&345			&343\\ \hline
MANN\_a81			&	1100		&1084		&1100			&1100			&1095\\ \hline
brock200\_2			&	12			&12			&12				&12				&12\\ \hline
brock200\_4			&	17			&16			&17				&17				&17\\ \hline
brock400\_2			&	29			&24			&29				&29				&29\\ \hline
brock400\_4			&	33			&25			&33				&33				&33\\ \hline
brock800\_2			&	24			&20			&24				&21				&21\\ \hline
brock800\_4			&	26			&19			&26				&21				&21\\ \hline
gen200\_p0.9\_44	&	44			&44			&44				&44				&44\\ \hline
gen200\_p0.9\_55	&	55			&55			&55				&55				&55\\ \hline
gen400\_p0.9\_55	&	55			&50			&55				&55				&53\\ \hline
gen400\_p0.9\_65	&	65			&65			&65				&65				&65\\ \hline
gen400\_p0.9\_75	&	75			&75			&75				&75				&75\\ \hline
hamming10-4		&	40			&40			&40				&40				&40\\ \hline
hamming8-4			&	16			&16			&16				&16				&16\\ \hline
keller4				&	11			&11			&11				&11				&11\\ \hline
keller5				&	27			&27			&27				&27				&27\\ \hline
keller6				&	59			&52			&59				&59				&57\\ \hline
p\_hat300-1			&	8			&8 			&8				&8				&8\\ \hline
p\_hat300-2			&	25			&25			&25				&25				&25\\ \hline
p\_hat300-3			&	36			&36			&36				&36				&36\\ \hline
p\_hat700-1			&	11			&11			&11				&11				&11\\ \hline
p\_hat700-2			&	44			&44			&44				&44				&44\\ \hline
p\_hat700-3			&	62			&62			&62				&62				&62\\ \hline
p\_hat1500-1		&	12			&12			&12				&12				&12\\ \hline
p\_hat1500-2		&	65			&65			&65				&65				&65\\ \hline
p\_hat1500-3		&	94			&93			&94				&94				&94\\ \hline
    \hline
\end{tabular}
\caption{Tabela com os resultados do procedimento nas 37 instâncias do DIMACS}
\end{table}
\end{center}

\section{Conclusão}

Neste artigo foi apresentada uma implementação do Iterated Local Search no problema da clique máxima. Essa implementação focou na tomada de decisões aleatórias em todas as fases do algoritmo do ILS, utilizando duas vizinhanças diferentes para a busca local, $N_1$ e $N_2$, que consistem nas subcliques da clique que passará pela busca local de tamanho $k-1$ e $k-2$ respectivamente, onde $k$ é o tamanho da clique. A perturbação simplesmente retira uma quantidade aleatória de vértices da clique, e eventualmente reinicializa o processo.\par

Os resultados, como esperado, mostram que aleatoriedade é pior do que utilizar uma heurística gulosa para a busca local. A implementação desse artigo encontra uma clique em média 10\% pior que a média do IKLS \cite{kopt} e a maior discrepância encontrada é de 27\% pior que o IKLS. Porém, este algoritmo errou, em geral, apenas nas instâncias mais difíceis.\par

Trabalhos futuros envolvem testar diversas diferentes buscas locais e perturbações, de maneira a encontrar a melhor possível forma do ILS para o problema da clique máxima. Além disso, uma comparação da melhor forma encontrada com outros tipos de mataheurísticas, como algoritmos genéticos, GRASP, evolutivos entre outros. Além disso, encontrar uma maneira mais eficiente de se calcular o $N_{\{C-v\}}$ de uma clique $C$ que foi retirado o nó $v$, pois a forma atual faz com que o algoritmo seja muito lento.

\begin{thebibliography}{58}

\bibitem{kopt}
  Katayama, Kengo, Masashi Sadamatsu, and Hiroyuki Narihisa. 
"Iterated k-opt local search for the maximum clique problem." 
\textit{Lecture Notes in Computer Science} 4446 (2007): 84.

\bibitem{review}
Wu, Qinghua, and Jin-Kao Hao. 
"A review on algorithms for maximum clique problems." 
\textit{European Journal of Operational Research} 242.3 (2015): 693-709.

\bibitem{pardaloshand}
I.M. Bomze, M. Budinich, P.M. Pardalos, and M. Pelillo. The maximum clique
problem. In D.-Z. Du and P.M. Pardalos, editors, \textit{Handbook of Combinatorial
Optimization (suppl. Vol. A)}, pp. 1–74. Kluwer, 1999.

\bibitem{DIMACS2}
D.S. Johnson and M.A. Trick. \textit{Cliques, Coloring, and Satisfiability}. Second DIMACS
Implementation Challenge, DIMACS Series in Discrete Mathematics and
Theoretical Computer Science. American Mathematical Society, 1996.

\bibitem{KLS}
 K. Katayama, A. Hamamoto, and H. Narihisa. An effective local search for the
maximum clique problem. \textit{Information Processing Letters}, Vol. 95, No. 5, pp.
503–511, 2005.

\bibitem{handbook}
Glover, Fred W., and Gary A. Kochenberger, eds. \textit{Handbook of metaheuristics}. Vol. 57. Springer Science \& Business Media, 2006

\bibitem{git}
https://github.com/Clique33/TrabalhoIntComp.git Acesso em 04/07/2017.

\bibitem{vns}
P. Hansen, N. Mladenovic, and D. Urosevic. \textit{Variable neighborhood search for
the maximum clique}. Discrete Applied Mathematics, Vol. 145, No. 1, pp. 117–125,
2004

\bibitem{HSSGA}
A. Singh and A. K. Gupta. A hybrid heuristic for the maximum clique problem.
\textit{Journal of Heuristics}, Vol. 12, No. 1-2, pp. 5–22, 2006.

\end{thebibliography}

\end{document}